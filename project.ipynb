{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "from textblob import Word\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load NLTK resources and Spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Set your Kaggle username and API key\n",
    "os.environ['KAGGLE_USERNAME'] = \"your_kaggle_username\"\n",
    "os.environ['KAGGLE_KEY'] = \"your_kaggle_api_key\"\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "!kaggle datasets download -d kaggle_username/dataset_name\n",
    "\n",
    "# Unzip the dataset\n",
    "with ZipFile('dataset_name.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')\n",
    "\n",
    "# Load data into a pandas DataFrame\n",
    "df = pd.read_csv('data/job_summary.csv')\n",
    "\n",
    "\n",
    "# Define methods dictionary for each stage\n",
    "methods = {\n",
    "    \"preprocessing\": {\n",
    "        \"remove_noise\": [remove_noise_regex, remove_noise_html, remove_numbers, remove_punctuation],\n",
    "        \"lowercase\": [to_lowercase, to_lowercase_spacy],\n",
    "        \"remove_stopwords\": [remove_stopwords_nltk, remove_stopwords_spacy],\n",
    "        \"lemmatize\": [lemmatize_nltk, lemmatize_spacy, lemmatize_textblob],\n",
    "        \"stem\": [stem_porter, stem_snowball, stem_lancaster]\n",
    "    },\n",
    "    \"feature_extraction\": {\n",
    "        \"tf_idf\": [tf_idf_feature_extraction, tf_idf_spacy],\n",
    "        \"bag_of_words\": [bag_of_words_feature_extraction]\n",
    "    },\n",
    "    \"similarity_detection\": {\n",
    "        \"cosine_similarity\": [cosine_similarity],\n",
    "        \"jaccard_similarity\": [jaccard_similarity],\n",
    "        \"edit_distance\": [edit_distance]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Measure time function\n",
    "def measure_time(func, text_series):\n",
    "    start_time = time.time()\n",
    "    text_series.apply(func)\n",
    "    return time.time() - start_time\n",
    "\n",
    "# Preprocessing functions\n",
    "def remove_noise_regex(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_noise_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def to_lowercase_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.text.lower() for token in doc])\n",
    "\n",
    "def remove_stopwords_nltk(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def remove_stopwords_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def lemmatize_nltk(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def lemmatize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def lemmatize_textblob(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [Word(word).lemmatize() for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def stem_porter(text):\n",
    "    words = word_tokenize(text)\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def stem_snowball(text):\n",
    "    words = word_tokenize(text)\n",
    "    snowball_stemmer = SnowballStemmer(language='english')\n",
    "    stemmed_words = [snowball_stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def stem_lancaster(text):\n",
    "    words = word_tokenize(text)\n",
    "    lancaster_stemmer = LancasterStemmer()\n",
    "    stemmed_words = [lancaster_stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Feature extraction functions\n",
    "def tf_idf_feature_extraction(text_data):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(text_data)\n",
    "    return X, vectorizer\n",
    "\n",
    "def tf_idf_spacy(text_data):\n",
    "    processed_text = [to_lowercase_spacy(doc) for doc in text_data]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(processed_text)\n",
    "    return X, vectorizer\n",
    "\n",
    "def bag_of_words_feature_extraction(text_data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(text_data)\n",
    "    return X, vectorizer\n",
    "\n",
    "# Similarity detection functions\n",
    "def cosine_similarity(X):\n",
    "    return cosine_similarity(X)\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
    "\n",
    "def edit_distance(str1, str2):\n",
    "    return nltk.edit_distance(str1, str2)\n",
    "\n",
    "# Evaluate methods using cross-validation\n",
    "def evaluate_methods_cv(df, methods_dict, stage_name, k_folds=5):\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    best_methods = {}\n",
    "\n",
    "    methods = methods_dict[stage_name]\n",
    "\n",
    "    for method_name, funcs in methods.items():\n",
    "        best_f1_score = -1\n",
    "        best_func = None\n",
    "\n",
    "        for func in funcs:\n",
    "            f1_scores = []\n",
    "\n",
    "            for train_index, test_index in kf.split(df):\n",
    "                train_df = df.iloc[train_index]\n",
    "                test_df = df.iloc[test_index]\n",
    "\n",
    "                cleaned_texts = train_df['job_summary'].apply(func)\n",
    "\n",
    "                # Example of feature extraction and similarity detection, adjust based on Our method\n",
    "                feature_vectors, _ = best_feature_methods[\"feature_extraction\"](cleaned_texts)\n",
    "                similarity_matrix = best_similarity_methods[\"similarity_detection\"](feature_vectors)\n",
    "\n",
    "                # Evaluate similarity detection performance\n",
    "                labels = np.zeros((len(test_df), len(test_df)))\n",
    "                for i in range(len(test_df)):\n",
    "                    for j in range(i+1, len(test_df)):\n",
    "                        if similarity_matrix[i][j] >= threshold:\n",
    "                            labels[i][j] = 1\n",
    "\n",
    "                ground_truth = np.zeros((len(test_df), len(test_df)))  # Dummy for illustration, replace with actual labels\n",
    "\n",
    "                f1 = f1_score(ground_truth.flatten(), labels.flatten())\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "            avg_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            if avg_f1_score > best_f1_score:\n",
    "                best_f1_score = avg_f1_score\n",
    "                best_func = func\n",
    "\n",
    "        best_methods[method_name] = best_func\n",
    "\n",
    "    return best_methods\n",
    "\n",
    "# Select best methods for each stage using cross-validation\n",
    "best_preprocessing_methods = evaluate_methods_cv(df, methods, \"preprocessing\")\n",
    "best_feature_extraction_methods = evaluate_methods_cv(df, methods, \"feature_extraction\")\n",
    "best_similarity_detection_methods = evaluate_methods_cv(df, methods, \"similarity_detection\")\n",
    "\n",
    "# Apply best methods to clean job summaries\n",
    "def apply_best_methods(text, methods_dict):\n",
    "    for stage_name, best_methods in methods_dict.items():\n",
    "        for method_name, best_func in best_methods.items():\n",
    "            text = best_func(text)\n",
    "    return text\n",
    "\n",
    "df['cleaned_summary'] = df['job_summary'].apply(lambda x: apply_best_methods(x, {\n",
    "    \"preprocessing\": best_preprocessing_methods,\n",
    "    \"feature_extraction\": best_feature_extraction_methods,\n",
    "    \"similarity_detection\": best_similarity_detection_methods\n",
    "}))\n",
    "\n",
    "# Display results\n",
    "print(\"Best methods selected using cross-validation for each stage:\")\n",
    "print(\"Preprocessing methods:\", best_preprocessing_methods)\n",
    "print(\"Feature extraction methods:\", best_feature_extraction_methods)\n",
    "print(\"Similarity detection methods:\", best_similarity_detection_methods)\n",
    "print()\n",
    "print(df[['job_summary', 'cleaned_summary']].head())\n",
    "\n",
    "# Example of Word2Vec embedding\n",
    "def word2vec_embedding(df):\n",
    "    tokenized_sentences = df['cleaned_summary'].apply(word_tokenize)\n",
    "    model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    \n",
    "    word_embedding = model.wv['engineer']  # Example of obtaining word embedding\n",
    "    print(\"Embedding vector for 'engineer':\", word_embedding)\n",
    "    \n",
    "    similarity = model.wv.similarity('engineer', 'developer')  # Example of calculating similarity\n",
    "    print(\"Similarity between 'engineer' and 'developer':\", similarity)\n",
    "\n",
    "# Execute Word2Vec embedding\n",
    "word2vec_embedding(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
